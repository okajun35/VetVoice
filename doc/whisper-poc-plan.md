# Whisper併用POC（精度検証フロー）

最終更新: 2026-02-26

## 目的

`Transcribe + Whisper` の multi-ASR 併用が、獣医療ドメインで実運用価値があるかを
低コストで判定する。

## 現状認識

- 現行パイプライン: `Transcribe -> 辞書/正規化 -> LLM整形`
- 課題: 専門語・略語（病名、薬剤、処置、繁殖ショートハンド）の取りこぼし
- 制約: Whisperを常時運用せず、月額コスト上限（POC目安: 200 USD）

## 結論（方針）

最初は本番統合せず、**オフライン比較POC** でよい。

- 対象音声: まず10本（一次判定）
- 同一音声で `Transcribe結果` と `Whisper結果` を取得
- 辞書適用後の精度を比較
- 改善が見えたら30〜50本に拡張し、統合実装を判断

## 期待改善レンジ（目安）

- 全体認識（WER/CER）: 相対 `+5〜15%`
- 専門語一致率（病名/薬剤/処置）: 絶対 `+10〜30pt`
- SOAP抽出（A/P）: `+5〜12pt`

注記:
- 設定固定（例: `temperature=0`）であれば、Whisper出力は再現性を確保しやすい。
- 10本は「手応え確認」用途。統計的な確証には30本以上が望ましい。

## 実装アプローチ比較（ざっくり）

1. ルールベース部分置換（Whisper優先）
- 難易度: 2/5
- 工数: 5〜8人日
- 備考: 早いが、境界条件の誤置換リスクあり

2. 疑似confusion network（推奨POC）
- 難易度: 3/5
- 工数: 10〜15人日
- 備考: 時刻合わせ + 信頼度重み投票で現実的

3. 本格lattice merging
- 難易度: 5/5
- 工数: 20〜30人日以上
- 備考: API制約下では重く、POC初期には非推奨

## 最小POC手順（10本）

1. 評価用の音声10本と正解テキスト（専門語ラベル付き）を用意
2. 各音声で `Transcribe` と `Whisper` の文字起こし結果を作成
3. 両結果に同一の辞書/正規化を適用
4. 指標を比較
   - 専門語一致率（病名/薬剤/処置）
   - SOAP A/P 正解率
   - 必要ならCER/WER
5. 判定
   - 目安: 専門語一致率が `+10pt` 以上なら次段階へ

## 次段階（改善が出た場合）

1. データセットを30〜50本に拡張して再評価
2. 夜間バッチ統合（S3入力 -> Whisper推論 -> 再抽出）を検討
3. 即時表示（現行Transcribe系）と夜間精緻化結果の2段運用を設計

